# Transformer æ‰‹å·¥å®ç°ä¸å®éªŒæŠ¥å‘Š

æœ¬é¡¹ç›®å®ç°äº†å®Œæ•´çš„ Transformer æ¨¡å‹ï¼ˆåŒ…å« Encoder å’Œ Decoderï¼‰ï¼Œå¹¶åœ¨å°è§„æ¨¡æ–‡æœ¬å»ºæ¨¡ä»»åŠ¡ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œæ¶ˆèå®éªŒã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ src/                    # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ model.py           # Transformeræ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ ablation_study.py  # æ¶ˆèå®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ data_loader.py     # æ•°æ®åŠ è½½å·¥å…·
â”‚   â””â”€â”€ utils.py           # å·¥å…·å‡½æ•°
â”œâ”€â”€ scripts/               # è¿è¡Œè„šæœ¬
â”‚   â””â”€â”€ run.sh            # è®­ç»ƒè¿è¡Œè„šæœ¬
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â””â”€â”€ tiny_shakespeare.txt  # æ•°æ®é›†
â”œâ”€â”€ results/               # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ exp_*/            # æ¯æ¬¡å®éªŒçš„ç»“æœ
â”‚   â””â”€â”€ ablation_study/   # æ¶ˆèå®éªŒç»“æœ
â”œâ”€â”€ requirements.txt      # Pythonä¾èµ–
â””â”€â”€ README.md            # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv transformer_env
source transformer_env/bin/activate  # Linux/Mac
# æˆ–
transformer_env\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. å‡†å¤‡æ•°æ®

ä¸‹è½½ Tiny Shakespeare æ•°æ®é›†ï¼š

```bash
# åˆ›å»ºdataç›®å½•
mkdir -p data

# ä¸‹è½½æ•°æ®é›†ï¼ˆLinux/Macï¼‰
wget -O data/tiny_shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

# Windowsç”¨æˆ·å¯ä»¥ä½¿ç”¨PowerShell
# Invoke-WebRequest -Uri "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt" -OutFile "data/tiny_shakespeare.txt"
```

æˆ–è€…æ‰‹åŠ¨ä¸‹è½½ï¼šhttps://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

### 3. è®­ç»ƒæ¨¡å‹

#### åŸºç¡€è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python src/train.py --data_path data/tiny_shakespeare.txt --use_cuda

# æˆ–ä½¿ç”¨è¿è¡Œè„šæœ¬ï¼ˆLinux/Macï¼‰
#bash scripts/run.sh
```

#### å®Œæ•´è®­ç»ƒå‘½ä»¤ï¼ˆå¯é‡ç°å®éªŒï¼‰

```bash
python src/train.py \
    --d_model 128 \
    --num_heads 4 \
    --num_encoder_layers 2 \
    --num_decoder_layers 2 \
    --d_ff 512 \
    --dropout 0.1 \
    --batch_size 32 \
    --learning_rate 3e-4 \
    --weight_decay 0.01 \
    --epochs 20 \
    --seq_length 50 \
    --seed 42 \
    --grad_clip 1.0 \
    --scheduler cosine \
    --data_path data/tiny_shakespeare.txt \
    --use_cuda
```

#### è¿è¡Œæ¶ˆèå®éªŒ

```bash
python src/ablation_study.py \
    --data_path data/tiny_shakespeare.txt \
    --epochs 20 \
    --seed 42 \
    --use_cuda
```

## âš™ï¸ å‚æ•°è¯´æ˜

### æ¨¡å‹å‚æ•°

- `--d_model`: æ¨¡å‹ç»´åº¦ï¼ˆé»˜è®¤ï¼š128ï¼‰
- `--num_heads`: æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤ï¼š4ï¼‰
- `--num_encoder_layers`: ç¼–ç å™¨å±‚æ•°ï¼ˆé»˜è®¤ï¼š2ï¼‰
- `--num_decoder_layers`: è§£ç å™¨å±‚æ•°ï¼ˆé»˜è®¤ï¼š2ï¼‰
- `--d_ff`: å‰é¦ˆç½‘ç»œç»´åº¦ï¼ˆé»˜è®¤ï¼š512ï¼‰
- `--dropout`: Dropoutç‡ï¼ˆé»˜è®¤ï¼š0.1ï¼‰

### è®­ç»ƒå‚æ•°

- `--batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š32ï¼‰
- `--learning_rate`: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ï¼š3e-4ï¼‰
- `--weight_decay`: æƒé‡è¡°å‡ï¼ˆé»˜è®¤ï¼š0.01ï¼‰
- `--epochs`: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ï¼š20ï¼‰
- `--seq_length`: åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ï¼š50ï¼‰
- `--seed`: éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰

### è®­ç»ƒç¨³å®šæ€§æŠ€å·§

- `--grad_clip`: æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé»˜è®¤ï¼š1.0ï¼‰
- `--scheduler`: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼šcosine, step, warmup, noneï¼Œé»˜è®¤ï¼šcosineï¼‰
- `--step_size`: StepLRæ­¥é•¿ï¼ˆé»˜è®¤ï¼š10ï¼‰
- `--gamma`: StepLRè¡°å‡ç‡ï¼ˆé»˜è®¤ï¼š0.8ï¼‰
- `--warmup_steps`: Warmupæ­¥æ•°ï¼ˆé»˜è®¤ï¼š5ï¼‰

## ğŸ–¥ï¸ ç¡¬ä»¶è¦æ±‚

- **CPU**: æ”¯æŒå³å¯è¿è¡Œï¼ˆè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼‰
- **GPU**: æ¨èä½¿ç”¨ CUDA æ”¯æŒçš„ GPUï¼ˆè®­ç»ƒé€Ÿåº¦æ˜¾è‘—æå‡ï¼‰
  - æ˜¾å­˜è¦æ±‚ï¼šè‡³å°‘ 2GBï¼ˆbatch_size=32, d_model=128ï¼‰
  - æµ‹è¯•ç¯å¢ƒï¼šNVIDIA GPU with CUDA 11.0+

## ğŸ“Š å®éªŒç»“æœ

è®­ç»ƒå®Œæˆåï¼Œç»“æœä¼šä¿å­˜åœ¨ `results/exp_YYYYMMDD_HHMMSS/` ç›®å½•ä¸‹ï¼š

- `best_model.pth`: æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
- `final_model.pth`: æœ€ç»ˆæ¨¡å‹æ£€æŸ¥ç‚¹
- `training_curves.png`: è®­ç»ƒæ›²çº¿å›¾
- `results.json`: è®­ç»ƒç»“æœæ•°æ®
- `config.json`: å®éªŒé…ç½®
- `vocab.json`: è¯æ±‡è¡¨

### æ¶ˆèå®éªŒç»“æœ

æ¶ˆèå®éªŒç»“æœä¿å­˜åœ¨ `results/ablation_study/` ç›®å½•ä¸‹ï¼š

- `ablation_comparison.png`: å®éªŒç»“æœå¯¹æ¯”å›¾
- `ablation_results.csv`: ç»“æœè¡¨æ ¼
- `ablation_results.json`: è¯¦ç»†ç»“æœæ•°æ®

## ğŸ”¬ å®ç°ç‰¹æ€§

### æ ¸å¿ƒç»„ä»¶

âœ… **Multi-Head Self-Attention**: å®Œæ•´å®ç°ï¼ŒåŒ…å«ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶  
âœ… **Position-wise FFN**: ä½ç½®å‰é¦ˆç½‘ç»œï¼Œæ”¯æŒ GELU/ReLU æ¿€æ´»  
âœ… **æ®‹å·®è¿æ¥ + LayerNorm**: Pre-LN æ¶æ„  
âœ… **ä½ç½®ç¼–ç **: æ­£å¼¦ä½ç½®ç¼–ç   

### Encoder-Decoder æ¶æ„

âœ… **Encoder Block**: åŒ…å«è‡ªæ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç½‘ç»œå±‚  
âœ… **Decoder Block**: åŒ…å«æ©ç è‡ªæ³¨æ„åŠ›ã€äº¤å‰æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œå±‚  
âœ… **å®Œæ•´ Transformer**: ç«¯åˆ°ç«¯çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹  

### è®­ç»ƒç¨³å®šæ€§æŠ€å·§

âœ… **AdamW ä¼˜åŒ–å™¨**: å¸¦æƒé‡è¡°å‡çš„ Adam ä¼˜åŒ–å™¨  
âœ… **å­¦ä¹ ç‡è°ƒåº¦**: æ”¯æŒ Cosineã€Stepã€Warmup è°ƒåº¦å™¨  
âœ… **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸  
âœ… **Dropout**: æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ  

### å®éªŒåŠŸèƒ½

âœ… **è®­ç»ƒæ›²çº¿å¯è§†åŒ–**: è‡ªåŠ¨ç”ŸæˆæŸå¤±å’Œå­¦ä¹ ç‡æ›²çº¿  
âœ… **æ¨¡å‹ä¿å­˜/åŠ è½½**: æ”¯æŒæ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤  
âœ… **å‚æ•°ç»Ÿè®¡**: è‡ªåŠ¨ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡  
âœ… **æ¶ˆèå®éªŒ**: ç³»ç»ŸåŒ–çš„æ¶ˆèç ”ç©¶æ¡†æ¶  

## ğŸ“ ä»£ç è¯´æ˜

### å…³é”®å®ç°ç‰‡æ®µ

#### Multi-Head Attention

```python
# ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
attn_weights = F.softmax(scores, dim=-1)
output = torch.matmul(attn_weights, V)
```

#### ä½ç½®ç¼–ç 

```python
# æ­£å¼¦ä½ç½®ç¼–ç 
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

#### æ®‹å·®è¿æ¥ + LayerNorm

```python
# Pre-LN æ¶æ„
attn_output, _ = self.self_attn(x, x, x, mask)
x = self.norm1(x + self.dropout(attn_output))
```

è¯¦ç»†å®ç°è¯·å‚è€ƒ `src/model.py`ã€‚

## ğŸ“š æ•°æ®é›†

æœ¬é¡¹ç›®ä½¿ç”¨ **Tiny Shakespeare** æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå°è§„æ¨¡çš„æ–‡æœ¬æ•°æ®é›†ï¼Œé€‚åˆå¿«é€Ÿå®éªŒå’ŒéªŒè¯ã€‚

- **æ¥æº**: https://github.com/karpathy/char-rnn
- **å¤§å°**: çº¦ 1MB
- **å†…å®¹**: èå£«æ¯”äºšä½œå“é›†

## ğŸ”„ å¯é‡ç°æ€§

æ‰€æœ‰å®éªŒéƒ½ä½¿ç”¨å›ºå®šéšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰ä»¥ç¡®ä¿å¯é‡ç°æ€§ã€‚ä½¿ç”¨ç›¸åŒçš„å‘½ä»¤å’Œå‚æ•°å¯ä»¥å¤ç°å®éªŒç»“æœã€‚


